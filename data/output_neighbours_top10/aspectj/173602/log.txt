Iteration: 1 trainLoss:0.5168854888232679 testLoss:0.3892050910451286
Iteration: 2 trainLoss:0.5018046938949764 testLoss:0.3796909308045233 trainLossDiff:-0.015080794928291508 testLossDiff:-0.009514160240605307
Iteration: 3 trainLoss:0.4912496614557487 testLoss:0.3720156177815335 trainLossDiff:-0.010555032439227674 testLossDiff:-0.007675313022989794
Iteration: 4 trainLoss:0.48354185198315724 testLoss:0.36836562237892134 trainLossDiff:-0.007707809472591476 testLossDiff:-0.003649995402612183
Iteration: 5 trainLoss:0.47767240269085165 testLoss:0.36363250216432674 trainLossDiff:-0.005869449292305595 testLossDiff:-0.004733120214594599
Iteration: 6 trainLoss:0.47308928245370796 testLoss:0.35987119841279375 trainLossDiff:-0.00458312023714369 testLossDiff:-0.003761303751532996
Iteration: 7 trainLoss:0.4693979764190651 testLoss:0.35708971293615266 trainLossDiff:-0.0036913060346428495 testLossDiff:-0.002781485476641088
Iteration: 8 trainLoss:0.4663590601358655 testLoss:0.35401510778266254 trainLossDiff:-0.003038916283199633 testLossDiff:-0.0030746051534901175
Iteration: 9 trainLoss:0.46378098730336315 testLoss:0.34634430385985593 trainLossDiff:-0.0025780728325023317 testLossDiff:-0.007670803922806613
Iteration: 10 trainLoss:0.4616068715088167 testLoss:0.34426737373399896 trainLossDiff:-0.0021741157945464606 testLossDiff:-0.0020769301258569683
Iteration: 11 trainLoss:0.4596957235304058 testLoss:0.3397388964675123 trainLossDiff:-0.0019111479784109098 testLossDiff:-0.004528477266486641
Iteration: 12 trainLoss:0.458038381574155 testLoss:0.336714696234808 trainLossDiff:-0.0016573419562507796 testLossDiff:-0.0030242002327043327
Iteration: 13 trainLoss:0.45654523085465026 testLoss:0.3339927305602335 trainLossDiff:-0.0014931507195047322 testLossDiff:-0.0027219656745745002
Iteration: 14 trainLoss:0.45520807187969836 testLoss:0.32999660032979106 trainLossDiff:-0.001337158974951902 testLossDiff:-0.0039961302304424295
Iteration: 15 trainLoss:0.45399222701408937 testLoss:0.3272318078762007 trainLossDiff:-0.0012158448656089949 testLossDiff:-0.002764792453590348
Iteration: 16 trainLoss:0.45289476491931097 testLoss:0.32272260613329096 trainLossDiff:-0.0010974620947784008 testLossDiff:-0.004509201742909752
Iteration: 17 trainLoss:0.4519025005708798 testLoss:0.3217847381016616 trainLossDiff:-9.922643484311933E-4 testLossDiff:-9.378680316293742E-4
Iteration: 18 trainLoss:0.45098405809134917 testLoss:0.3193402439973846 trainLossDiff:-9.184424795306012E-4 testLossDiff:-0.002444494104276995
Iteration: 19 trainLoss:0.45013103356756873 testLoss:0.3160508629881358 trainLossDiff:-8.530245237804435E-4 testLossDiff:-0.003289381009248804
Iteration: 20 trainLoss:0.44934444589927 testLoss:0.3129699312559648 trainLossDiff:-7.865876682987549E-4 testLossDiff:-0.0030809317321709995
Iteration: 21 trainLoss:0.4486182448337634 testLoss:0.31175274380444296 trainLossDiff:-7.26201065506582E-4 testLossDiff:-0.0012171874515218284
Iteration: 22 trainLoss:0.44795818901462836 testLoss:0.3102946271679943 trainLossDiff:-6.600558191350303E-4 testLossDiff:-0.0014581166364486808
Iteration: 23 trainLoss:0.4473372714297672 testLoss:0.30807772490361934 trainLossDiff:-6.209175848611581E-4 testLossDiff:-0.0022169022643749314
Iteration: 24 trainLoss:0.44680347781845187 testLoss:0.30794301858455236 trainLossDiff:-5.337936113153363E-4 testLossDiff:-1.347063190669795E-4
Iteration: 25 trainLoss:0.44621715522572514 testLoss:0.30330238145368216 trainLossDiff:-5.86322592726729E-4 testLossDiff:-0.004640637130870207
Iteration: 26 trainLoss:0.4457162997867804 testLoss:0.30173223633541907 trainLossDiff:-5.008554389447362E-4 testLossDiff:-0.0015701451182630888
Iteration: 27 trainLoss:0.4452490294643849 testLoss:0.3010853271583712 trainLossDiff:-4.672703223955188E-4 testLossDiff:-6.469091770478497E-4
Iteration: 28 trainLoss:0.44480447318256433 testLoss:0.29877554591309935 trainLossDiff:-4.4455628182055174E-4 testLossDiff:-0.002309781245271869
Iteration: 29 trainLoss:0.4443947730512824 testLoss:0.29873796624864896 trainLossDiff:-4.097001312819426E-4 testLossDiff:-3.75796644503934E-5
Iteration: 30 trainLoss:0.4439954898883465 testLoss:0.2964528944733661 trainLossDiff:-3.9928316293591504E-4 testLossDiff:-0.0022850717752828653
Learned weights: -38.47639721617188 44.15916694355948 2.553160234297016
